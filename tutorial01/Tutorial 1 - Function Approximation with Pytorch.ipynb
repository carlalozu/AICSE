{"cells":[{"cell_type":"markdown","metadata":{"id":"l2SbQ9mHqUx-","pycharm":{"name":"#%% md\n"}},"source":["# Function Approximation with PyTorch\n","\n","The objective of this tutorial is learn the basics of PyTorch.\n","\n","To this end, we cosider the task of learning the function\n","\n","$$u = sin(x), \\quad x\\in(0, 2\\pi)$$\n","\n","by using feed-forward dense neural networks."]},{"cell_type":"markdown","metadata":{"id":"sNmKayODqUyA","pycharm":{"name":"#%% md\n"}},"source":["$\\mathbf{\\text{Step 1: Dataset Generation}}$\n","\n","We are going to use the training set\n","\n","$$S=\\{ (x_i, u_i), ~i=1,...,n \\}$$\n","\n","with\n","\n","$$x_i \\sim U\\big(0, 2\\pi\\big), \\quad u_i \\sim N\\big(sin(x_i), \\sigma(x_i)\\big).$$\n","\n","Here, $U(a,b)$ is a uniform random distribution between $a$ and $b$, and $N(\\mu, \\sigma)$ is a normal distribution with mean $\\mu$ and standard deviation $\\sigma$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"111sRdoleTML"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cUQdfjjVew1P"},"outputs":[],"source":["# Uncomment and complete the following code to generate synthetic sine wave data with added noise\n","def generate_data(num_points, noise_std=0.1):\n","    # Generate random x values\n","    x_values = np.sort(np.random.uniform(0, 2 * np.pi, num_points))\n","    # Generate corresponding y values for sine curve with added noise\n","    y_values = np.sin(x_values) + np.random.normal(0, noise_std, num_points)\n","    return x_values, y_values\n","\n","num_points = 1000\n","\n","# You can adjust the noise level as needed\n","x_train, y_train = generate_data(num_points, noise_std=0.1)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":564},"executionInfo":{"elapsed":795,"status":"ok","timestamp":1708854237222,"user":{"displayName":"Victor Armegioiu","userId":"10988735183621484296"},"user_tz":-60},"id":"YdtRv-OlfGJt","outputId":"fd2b48d9-14d0-4e21-f5d4-ee5cd9e302a5"},"outputs":[],"source":["# Plot the generated data\n","plt.figure(figsize=(10, 6))\n","\n","plt.scatter(\n","    x_train, y_train,\n","    label='Generated Data with Noise', color='blue', alpha=0.5)\n","plt.plot(\n","    x_train, np.sin(x_train),\n","    label='True Sine Curve', color='red', linestyle='--')\n","\n","plt.legend()\n","plt.title('Generated Sine Wave Data with Noise')\n","plt.xlabel('X values')\n","plt.ylabel('Y values')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"0FhWveHAqUyB","pycharm":{"name":"#%% md\n"}},"source":["$\\mathbf{\\text{Step 2: Model Definition}}$\n","\n","We use feedforward neural (also named as a multi-layer perceptron) network to approximate the function\n","\n","$$ u = sin(x)$$\n","\n","Given an input $x \\in D \\subset R^n$, a feedforward neural network transforms it to an output $u_\\theta(x)\\in R^m$, through a layer of units (neurons) which compose of either affine-linear maps between units (in successive layers) or scalar non-linear activation functions within units, resulting in the representation,\n","\n","$$u_{\\theta}(x) = C_K \\circ A \\circ C_{K-1}\\ldots \\ldots \\ldots \\circ A\\circ C_2 \\circ A\\circ C_1(x).$$\n","\n","Here, $\\circ$ refers to the composition of functions and $A$ is a scalar (non-linear) activation function. For any $1 \\leq k \\leq K$, we define\n","\n","$$\n","C_k z_k = W_k z_k + b_k, \\quad {\\rm for} ~ W_k \\in R^{d_{k+1} \\times d_k}, z_k \\in R^{d_k}, b_k \\in R^{d_{k+1}}.\n","$$\n","\n","We also denote,\n","$$\n","\\theta = \\{W_k, b_k\\}, \\theta_W = \\{ W_k \\}\\quad \\forall~ 1 \\leq k \\leq K,\n","$$\n","to be the complete set of (tunable) weights for our network.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HN1gKeqkqUyC","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["class SimpleNN(nn.Module):\n","    def __init__(self):\n","        super(SimpleNN, self).__init__()\n","        # TODO: Define your model weights.\n","        self.conv1 = nn.Conv1d(in_channels=1, out_channels=20, kernel_size=5, padding=2)\n","        self.conv2 = nn.Conv1d(in_channels=20, out_channels=1, kernel_size=5, padding=2)\n","\n","\n","    def forward(self, x):\n","        # TODO: Implement the forward pass.\n","        x = nn.functional.relu(self.conv1(x))\n","        x = nn.functional.relu(self.conv2(x))\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class SimpleNN(nn.Module):\n","    def __init__(self):\n","        super(SimpleNN, self).__init__()\n","        self.fc1 = nn.Linear(1, 10)  # 1 input feature, 10 hidden units\n","        self.activation = nn.SiLU()\n","        self.fc2 = nn.Linear(10, 1)  # 10 hidden units, 1 output\n","\n","    def forward(self, x):\n","        x = self.fc1(x)\n","        x = self.activation(x)\n","        x = self.fc2(x)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["x_train_ = torch.tensor(x_train, dtype=torch.float32).view(-1, 1)\n","x_pred = SimpleNN()(x_train_)"]},{"cell_type":"markdown","metadata":{"id":"WYXmofxIqUyD","pycharm":{"name":"#%% md\n"}},"source":["$\\mathbf{\\text{Step 3: Model Training}}$\n","\n","The neural network $u_{\\theta}$ depends on the tuning parameter $\\theta \\in \\Theta$ of weights and biases. Within the standard paradigm of deep learning, one trains the network by finding tuning parameters $\\theta$ such that a suitable loss function $J(\\theta)$ is minimized.\n","\n","$${\\rm Find}~\\theta^{\\ast} \\in \\Theta:\\quad \\theta^{\\ast} = {\\rm arg}\\min\\limits_{\\theta \\in \\Theta} \\left( J(\\theta)\\right).$$\n","\n","The loss function, for instance, can be chosen as the mean square error between the neural network and the underlying target:\n","\n","$$ J(\\theta) = \\sum_{i}^n \\Big(u_i - u_\\theta(x_i)\\Big)^2$$\n","\n","The optimization process is realized with the gradient descent (or more precisely with variants of the gradient descent such as Adam or SGD).\n","\n","1. Compute the loss function over the batch $j$:\n","$J_S(\\theta)=\\sum_{x_i \\in {S}_j}^n \\Big(u_i - u_\\theta(x_i)\\Big)^2$\n","\n","2. Compute the gradient of $J_S(\\theta)$ with respect to the network parameters:  $\\nabla_\\theta J_S(\\theta)$\n","\n","3. Update the parameters according to the chosen optimizer, for instance for minibatch stochastic gradient descent $\\theta_{k+1} = \\theta_{k} - \\eta \\nabla_\\theta J_S(\\theta_{k}) $ with $k=1,...,(n_{epoch} n_{batch})$ and $\\eta$ being the learning rate (argument $lr$ in the optimizer)."]},{"cell_type":"markdown","metadata":{"id":"UxgATTQtqUyD","pycharm":{"name":"#%% md\n"}},"source":["#### **ADAM Optimizer**\n","\n","Adaptive Momement Estimation: adaptive learning rates for different parameters from estimates of first and second moments of the gradients.\n","https://arxiv.org/pdf/1412.6980.pdf\n","\n","#### **LBFGS Optimizer (Generic Idea)**\n","\n","Let us consider a generic function $f: x\\mapsto f(x)$, $x\\in R^d$ and $f(x)\\in R$. The taylor approximation of $\\nabla f$ in a neighbourhood of $x_k$ is\n","\n","$$\\nabla f(x_k + \\delta x) = \\nabla f(x_k) + H_k\\delta x$$\n","\n","with $H_k$ being the Hessian of $f$ at $x_k$.\n","We would like to find the vector $ \\delta x$ such that\n","\n","$$\\nabla f(x_k + \\delta x) = 0$$\n","\n","This leads to\n","\n","$$x_{k+1} = x_k - H_k^{-1}\\nabla f \\quad \\text{(Newton Method)}$$\n","\n","The idea of Quasi-Newton methods (LBFGS belong to this class) is to approximate $H$ with a positive definite matrix $B$, which is easier to compute:\n","\n","$$x_{k+1} = x_k   - B_{k}^{-1}\\nabla f \\quad \\text{(Quasi - Newton Method)}$$\n","\n","The matrix B has to satisfy the following conditions:\n","- Positive definiteness\n","- $B_k$ and $B_{k+1}$ \"sufficiently close\"\n","- Secant equation  $$ B_{k+1}[x_{k+1} - x_k] = \\nabla f(x_{k+1}) - \\nabla f(x_{k})$$\n","\n","A constraint minimization problem can be set to find the update formula for $B_k$\n","(check https://towardsdatascience.com/bfgs-in-a-nutshell-an-introduction-to-quasi-newton-methods-21b0e13ee504)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1708947054496,"user":{"displayName":"Victor Armegioiu","userId":"10988735183621484296"},"user_tz":-60},"id":"k3sOouTUiYm4"},"outputs":[],"source":["def get_optimizer(optimizer_name, model, lr = 0.01, momentum = 0.9):\n","  optimizer = ...\n","  if optimizer_name == \"ADAM\":\n","      # TODO: Set up ADAM.\n","      optimizer = optim.Adam(model.parameters(), lr)\n","  elif optimizer_name == \"LBFGS\":\n","      # TODO: Set up LBFGS.\n","      optimizer = optim.LBFGS(model.parameters(), lr)\n","  else:\n","      raise ValueError(\"Optimizer not recognized\")\n","  return optimizer\n","\n","\n","\n","def train(model, optimizer, criterion, inputs, targets):\n","\n","    def closure():\n","        # TODO: Implement the optimization step logic.\n","        # 1. Zero out the gradient data saved in the optimizer.\n","        optimizer.zero_grad()\n","        # 2. Get predictions of the model.\n","        pred_y = model(inputs)\n","        # 3. Evaluate the loss function given your predictions and the targets.\n","        loss = criterion(pred_y, targets)\n","        # 4. Backpropagate the loss and return it.\n","        loss.backward()\n","\n","        return loss\n","\n","    optimizer.step(closure)\n","    return closure().item()\n","\n","\n","def evaluate(model, x_test):\n","    model.eval()\n","    with torch.no_grad():\n","        y_pred = model(x_test.view(-1, 1)).numpy()\n","    return y_pred"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"Shape of x_train:\", x_train.shape)\n","print(\"Shape of y_train:\", y_train.shape)"]},{"cell_type":"markdown","metadata":{"id":"iYxibsHSia8s"},"source":["#### Instantiate your model, define your loss function + optimizer, and perform the training procedure."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MjFZwW6m-ajq"},"outputs":[],"source":["# TODO: instantiate model, loss, and your optimizer.\n","model = SimpleNN()\n","criterion = torch.nn.MSELoss(reduction='sum')\n","\n","# Choose the optimizer (either Adam or LBFGS)\n","optimizer = get_optimizer(\"ADAM\", model, lr=0.01, momentum=0.9)\n","\n","# Start training the neural net.\n","num_epochs = 10000\n","for epoch in range(num_epochs):\n","    # Convert data to PyTorch tensors\n","    inputs = torch.tensor(\n","        x_train, dtype=torch.float32, requires_grad=True).view(-1, 1)\n","    targets = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n","\n","    # Training step.\n","    loss = train(model, optimizer, criterion, inputs, targets)\n","\n","    if (epoch+1) % 100 == 0:\n","        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss:.4f}')"]},{"cell_type":"markdown","metadata":{"id":"j-4NqJLcg0Ok"},"source":["$\\mathbf{\\text{Step 4: Plotting Predictions}}$\n","\n","\n","Final step, evaluate your predictions at some input locations and do some plotting.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ILvbYYuN-btE"},"outputs":[],"source":["x_test = torch.linspace(0, 2*np.pi, 100)\n","y_pred = evaluate(model, x_test)\n","\n","plt.figure(figsize=(10, 6))\n","plt.scatter(x_train, y_train, label='Original Sine Curve', color='blue')\n","plt.plot(x_test, y_pred, label='Learned Curve', color='red')\n","\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["y_pred"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.15"}},"nbformat":4,"nbformat_minor":0}
